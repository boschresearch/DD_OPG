#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Copyright (C) 2019 Robert Bosch GmbH

This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Affero General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Affero General Public License for more details.

You should have received a copy of the GNU Affero General Public License
along with this program. If not, see <https://www.gnu.org/licenses/>.

@author: Andreas Doerr
"""

import time

import numpy as np
import tensorflow as tf

from garage.algos import RLAlgorithm
from garage.misc import logger

from baselines.common.tf_util import flatgrad

from policy_gradients.optimizers import FirstOrderOptimizer

from policy_gradients.samplers import Sampler

from policy_gradients.models import DDOPGModel
from policy_gradients.models import SoftmaxMemorySelection


class DDOPG(RLAlgorithm):

    def __init__(self,
                 env,
                 policy_cls,
                 policy_args,
                 n_itr=1000,
                 max_path_length=100,
                 model_selection=False,
                 explore_thresh=0.6,
                 explore_width=2.0,
                 explore_limit=0.5,
                 reset=False):

        self.env = env

        self.policy_cls = policy_cls
        self.policy_args = policy_args

        self.n_itr = n_itr
        self.max_path_length = max_path_length

        self.rollout_params = None

        self.all_paths = []
        self.all_params = []
        self.all_Rs = []
        self.all_Hs = []

        # Objective
        self.ess_penalty = 0.05

        # Surrogate model
        self.model_log_std = 3.0
        self.model = DDOPGModel(policy_cls=self.policy_cls,
                                policy_args=self.policy_args,
                                max_path_length=self.max_path_length,
                                log_std=self.model_log_std,
                                delta=0.1,
                                return_prior_max=1000)

        # Sample paths from environment + policy
        self.sampler = Sampler(env=self.env,
                               policy_cls=self.policy_cls,
                               policy_args=self.policy_args,
                               max_path_length=self.max_path_length,
                               deterministic=True)

        # Memory selection configuration
        self.softmax_temp = 0.1
        self.max_paths = 50
        self.n_hist = 3
        self.memory_selection = SoftmaxMemorySelection(max_paths=self.max_paths,
                                                       softmax_temp=self.softmax_temp,
                                                       n_hist=self.n_hist)
        self.softmax_selection = SoftmaxMemorySelection(max_paths=self.max_paths,
                                                        softmax_temp=self.softmax_temp,
                                                        n_hist=0)

        # Exploration
        self.explore_thresh = explore_thresh
        self.explore_width = explore_width
        self.explore_limit = explore_limit
        self.reset = reset

        # Policy evaluation
        self.best_policy_params = None
        self.best_R = -np.inf
        self.avg_ret_best_policy = 0.0
        self.evaluation_interval = 10

        self._build_graph()

        self.optimizer = FirstOrderOptimizer(None,
                                             self.model.n_params)

    # -------------------------------------------------------------------------
    # Public methods
    # -------------------------------------------------------------------------

    def train(self):
        self._initiate_training()

        for itr in range(self.n_itr):
            self._training_step(itr)

        logger.log('Terminate training')
        self.sess.close()

    # -------------------------------------------------------------------------
    # Private methods
    # -------------------------------------------------------------------------

    def _initiate_training(self):
        """
        Initiates the training process by creating a tensorflow session
        and sampling 5 differnt policies which are generated by perturbing
        the initial policies parameters with random gaussian noise.
        """

        logger.log('Initiate training...')

        self.start_time = time.time()

        self.sess = tf.Session()
        self.sess.__enter__()
        self.sess.run(tf.global_variables_initializer())

        current_mean = self.model.policy.get_param_values(trainable=True)
        current_std = 1.0
        self.rollout_params = np.random.normal(loc=current_mean,
                                               scale=current_std,
                                               size=(5, self.model.n_params))

    def _training_step(self, itr):
        itr_start_time = time.time()

        with logger.prefix('itr #%d | ' % itr):
            self._sampling()

            self._bookkeeping()

            self._memory_selection(itr)
            self._policy_optimization(itr)

            if itr % self.evaluation_interval == 0:
                self._policy_evaluation()

            self._log_diagnostics(itr)

            logger.record_tabular('Time', time.time() - self.start_time)
            logger.record_tabular('ItrTime', time.time() - itr_start_time)
            logger.dump_tabular(with_prefix=False)

    def _sampling(self):
        logger.log('Obtaining samples...')
        self.paths = self.sampler.get_paths(self.rollout_params)

    def _bookkeeping(self):
        logger.log('Processing samples...')

        Rs = [sum(path['rewards']) for path in self.paths]
        Hs = [len(path['rewards']) for path in self.paths]
        current_params = [path['policy_params'] for path in self.paths]

        self.all_paths += self.paths
        self.all_params += current_params
        self.all_Rs += Rs
        self.all_Hs += Hs

        idx = np.argmax(Rs)
        if Rs[idx] > self.best_R:
            logger.log('Found new best policy parameters!...')
            self.best_R = Rs[idx]
            self.best_policy_params = current_params[idx].copy()

    def _memory_selection(self, itr):
        logger.log('Memory selection...')
        self.model_paths = self.memory_selection.select_paths_subset(self.all_paths, self.all_Rs)
        self.softmax_ids = self.softmax_selection.select_paths_subset(self.all_paths, self.all_Rs, return_indices=True)
        self.feed = self.model.get_feed(self.model_paths)

    def _policy_optimization(self, itr):
        logger.log('Policy optimization...')
        self._log_progress('Pre/')

        # Choose optimization starting parameters
        if itr == 0:
            x = self.all_params[np.argmax(self.all_Rs)]
        else:
            x = self.rollout_params.copy()

        # Define target objective based on surrogate model
        if itr == 0 or (np.max(self.all_Rs) / self.max_R) > 1.1:
            self.max_R = np.max(np.abs(self.all_Rs))

            var_list = self.model.policy.get_params(trainable=True)

            target = self.model.J_test / self.max_R + (self.model.ess_test / self.max_paths) * self.ess_penalty
            target_grad = flatgrad(target, var_list)

            def fun_and_jac(x):
                self.model.policy.set_param_values(x, trainable=True)
                return self.sess.run([target, target_grad], self.feed)

            self.fun_and_jac = fun_and_jac

        # Hard reset policy if stuck in bad local minimum
        if self.reset and itr > 10:
            sorted_R = sorted(self.all_Rs)
            cutoff = sorted_R[int(len(self.all_Rs)*self.explore_thresh)]
            print("Cutoff: ", cutoff)
            if self.all_Rs[-1] <= cutoff:
                loc = np.mean([self.all_params[idx] for idx in self.softmax_ids], axis=0)
                scale = np.std(self.all_params[-10:], axis=0) * self.explore_width
                scale = np.minimum(scale, self.explore_limit)
                x = np.random.normal(loc=loc, scale=scale, size=(self.model.n_params))
                print("_______________________________________________EXPLORING with mean scale: ", np.mean(scale))

        # SGD-based optimization
        self.optimizer.fun_and_jac = self.fun_and_jac
        x, f, fs = self.optimizer.optimize(x, step_size=1e-2*np.exp(self.model.log_std))
        self.rollout_params = x.copy()

        self._log_progress('Post/')

    def _policy_evaluation(self):
        """Evaluate the best policy parameters which have been seen so far for 10 iterations."""
        logger.log('Evaluating best policy...')
        test_paths = self.sampler.get_paths([self.best_policy_params] * 10)
        self.avg_ret_best_policy = np.mean([np.sum(path['rewards']) for path in test_paths])

    def _log_progress(self, prefix):
        loss_results = np.array(self.fcn_losses(self.feed))
        for (lossname, lossval) in zip(self.loss_names, loss_results):
            logger.record_tabular(prefix + lossname, lossval)

    def _log_diagnostics(self, itr):
        logger.log('Logging diagnostics...')

        undisc_returns = [sum(path["rewards"]) for path in self.paths]

        logger.record_tabular('Iteration', itr)
        logger.record_tabular('Paths/NumTrajs', len(self.paths))
        logger.record_tabular('Paths/Steps', np.mean([len(path['rewards']) for path in self.paths]))
        logger.record_tabular('Paths/Return/Best_Policy_Avg', self.avg_ret_best_policy)
        logger.record_tabular('Paths/Return/Mean', np.mean(undisc_returns))
        logger.record_tabular('Paths/Return/Std', np.std(undisc_returns))
        logger.record_tabular('Paths/Return/Max', np.max(undisc_returns))
        logger.record_tabular('Paths/Return/Min', np.min(undisc_returns))

        self.model.policy.log_diagnostics(self.paths)

    def _build_graph(self):

        losses = [self.model.J_test,
                  self.model.J2_test,
                  self.model.J_var_test,
                  self.model.J_unc_test,
                  self.model.ess_test]
        loss_names = ['J', 'J2', 'J_var', 'J_unc', 'ess']

        def fcn_losses(feed):
            return self.sess.run(losses, feed)

        self.fcn_losses = fcn_losses
        self.loss_names = loss_names
